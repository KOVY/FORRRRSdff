# ğŸ“‹ VS Code AI Workflow - KompletnÃ­ InstalaÄnÃ­ PrÅ¯vodce

## ğŸ¯ CÃ­l: RozÅ¡Ã­Å™enÃ­ stÃ¡vajÃ­cÃ­ho VS Code o pokroÄilÃ½ AI workflow

Tento prÅ¯vodce rozÅ¡Ã­Å™Ã­ tvÃ© stÃ¡vajÃ­cÃ­ VS Code prostÅ™edÃ­ o pokroÄilÃ© AI funkce, kterÃ© pÅ™evyÅ¡ujÃ­ Replit/Cursor moÅ¾nosti - a to zcela zdarma.

## âœ… PÅ™edpoklady (JiÅ¾ mÃ¡Å¡ nainstalovanÃ©)
- Ubuntu Desktop 22.04 LTS âœ…
- VS Code Desktop s zÃ¡kladnÃ­mi rozÅ¡Ã­Å™enÃ­mi âœ…
- Docker + Docker Compose âœ…
- Python 3.11 + Node.js 20.x âœ…
- PostgreSQL 15, Redis, Nginx âœ…
- Ollama s modely (Llama, DeepSeek V3, Qwen 3 2507) âœ…

---

## ğŸš€ Krok 1: Upgrade AI ModelÅ¯ pro VÃ½voj

### **PÅ™idÃ¡nÃ­ Qwen2.5-Coder (NejlepÅ¡Ã­ pro coding)**
```bash
# Zastavit Ollama pokud bÄ›Å¾Ã­
sudo systemctl stop ollama

# Aktualizovat Ollama na nejnovÄ›jÅ¡Ã­ verzi
curl -fsSL https://ollama.ai/install.sh | sh

# StÃ¡hnout nejnovÄ›jÅ¡Ã­ coding modely
ollama pull qwen2.5-coder:70b    # HlavnÃ­ coding model
ollama pull qwen2.5:70b          # Architect model (uÅ¾ mÃ¡Å¡)
ollama pull deepseek-v3          # Analyst model (uÅ¾ mÃ¡Å¡)

# OvÄ›Å™it staÅ¾enÃ© modely
ollama list
```

### **Instalace vLLM pro rychlejÅ¡Ã­ inference**
```bash
# Instalace vLLM s CUDA podporou (vyuÅ¾ije tvÃ© 6Ã— Nvidia P106-90)
pip install vllm torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Test GPU dostupnosti
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
```

---

## ğŸš€ Krok 2: Instalace AI Workflow Frameworku

### **Python AI Dependencies**
```bash
# HlavnÃ­ AI framework
pip install --upgrade \
  crewai \
  langgraph \
  mem0ai \
  litellm \
  anthropic \
  openai \
  aider-chat \
  continue-dev

# DatabÃ¡ze a vector store
pip install --upgrade \
  chromadb \
  pinecone-client \
  weaviate-client \
  qdrant-client

# Development tools
pip install --upgrade \
  fastapi \
  uvicorn \
  websockets \
  asyncio-mqtt \
  playwright
```

### **Node.js AI Tools**
```bash
# Model Context Protocol (MCP) framework
npm install -g \
  @modelcontextprotocol/sdk \
  @modelcontextprotocol/server-memory \
  @anthropic-ai/sdk

# Development tools
npm install -g \
  @railway/cli \
  vercel \
  tsx \
  concurrently \
  nodemon
```

---

## ğŸš€ Krok 3: VS Code RozÅ¡Ã­Å™enÃ­ pro AI Workflow

### **Instalace rozÅ¡Ã­Å™enÃ­ pÅ™es terminÃ¡l**
```bash
# AI a coding rozÅ¡Ã­Å™enÃ­
code --install-extension continue.continue
code --install-extension github.copilot-chat
code --install-extension ms-toolsai.jupyter
code --install-extension ms-python.python
code --install-extension ms-python.debugpy

# Advanced development
code --install-extension bradlc.vscode-tailwindcss
code --install-extension formulahendry.auto-rename-tag
code --install-extension christian-kohler.path-intellisense
code --install-extension christian-kohler.npm-intellisense
code --install-extension wix.vscode-import-cost

# Git a deployment
code --install-extension eamodio.gitlens
code --install-extension ms-vscode.vscode-github-issue-notebooks
code --install-extension github.vscode-github-actions

# Monitoring a debugging
code --install-extension ms-vscode.vscode-json
code --install-extension redhat.vscode-yaml
code --install-extension humao.rest-client
```

---

## ğŸš€ Krok 4: Konfigurace Continue.dev pro AI Modely

### **VytvoÅ™enÃ­ config souboru**
```bash
# VytvoÅ™ adresÃ¡Å™ pro Continue.dev config
mkdir -p ~/.continue

# VytvoÅ™ konfiguraÄnÃ­ soubor
cat > ~/.continue/config.json << 'EOF'
{
  "models": [
    {
      "title": "ğŸ¯ AI Architect (Qwen 3 2507)",
      "provider": "ollama",
      "model": "qwen2.5:70b",
      "apiBase": "http://localhost:11434",
      "contextLength": 128000,
      "systemMessage": "Jsi expert software architekt. NavrhujeÅ¡ Å¡kÃ¡lovatelnÃ©, udrÅ¾itelnÃ© systÃ©my s modernÃ­mi best practices. KomunikujeÅ¡ Äesky, ale kÃ³d pÃ­Å¡eÅ¡ v angliÄtinÄ›."
    },
    {
      "title": "ğŸ’» AI Coder (Qwen2.5-Coder)",
      "provider": "ollama", 
      "model": "qwen2.5-coder:70b",
      "apiBase": "http://localhost:11434",
      "contextLength": 32000,
      "systemMessage": "Jsi senior full-stack developer. PÃ­Å¡eÅ¡ ÄistÃ½, efektivnÃ­, produkÄnÃ­ kÃ³d s komplexnÃ­m error handlingem. KomunikujeÅ¡ Äesky, ale kÃ³d a komentÃ¡Å™e pÃ­Å¡eÅ¡ v angliÄtinÄ›."
    },
    {
      "title": "ğŸ§  AI Analyst (DeepSeek V3)",
      "provider": "ollama",
      "model": "deepseek-v3", 
      "apiBase": "http://localhost:11434",
      "contextLength": 64000,
      "systemMessage": "Jsi code reviewer a performance analytik. ZamÄ›Å™ujeÅ¡ se na optimalizaci, security a best practices. KomunikujeÅ¡ Äesky, ale doporuÄenÃ­ pÃ­Å¡eÅ¡ v angliÄtinÄ›."
    }
  ],
  "customCommands": [
    {
      "name": "ğŸš€ KompletnÃ­ Full-Stack Projekt",
      "prompt": "VytvoÅ™ kompletnÃ­ full-stack aplikaci s:\n- Modern React/TypeScript frontend\n- Node.js/Express backend s PostgreSQL\n- Autentifikace a autorizace\n- Responsive design s Tailwind CSS\n- Production deployment config pro Railway + Vercel\n- Comprehensive testing\n\nPoÅ¾adavky: {{{ input }}}"
    },
    {
      "name": "ğŸ”§ Optimalizace a Deploy",
      "prompt": "Optimalizuj tento kÃ³d pro produkci a vytvoÅ™ deployment konfiguraci pro Railway (backend) a Vercel (frontend). ZahrÅˆ environment variables, health checks a monitoring."
    },
    {
      "name": "ğŸ§ª GenerovÃ¡nÃ­ TestÅ¯",
      "prompt": "VytvoÅ™ comprehensive testy pro tento kÃ³d vÄetnÄ› unit testÅ¯, integration testÅ¯ a edge cases. PouÅ¾ij modernÃ­ testing frameworky a zajisti vysokÃ© pokrytÃ­."
    },
    {
      "name": "ğŸ“Š Performance Audit",
      "prompt": "Analyzuj tento kÃ³d na performance bottlenecky a navrhni konkrÃ©tnÃ­ optimalizace s mÄ›Å™itelnÃ½mi dopady na vÃ½kon."
    },
    {
      "name": "ğŸ”’ Security Review",
      "prompt": "ProveÄ security audit tohoto kÃ³du a navrhni zlepÅ¡enÃ­. ZamÄ›Å™ se na input validation, authentication, authorization a data protection."
    },
    {
      "name": "ğŸ“ Dokumentace",
      "prompt": "VytvoÅ™ comprehensive dokumentaci pro tento kÃ³d vÄetnÄ› README, API docs a inline komentÃ¡Å™Å¯. UdÄ›lej to beginner-friendly."
    },
    {
      "name": "ğŸ”„ Migration z Replit",
      "prompt": "PÅ™eveÄ tento Replit projekt na modernÃ­, lokÃ¡lnÄ› spustitelnÃ½ projekt s proper strukturou, dependencies a deployment konfiguracÃ­ pro Railway/Vercel."
    }
  ],
  "tabAutocompleteModel": {
    "title": "Tab Autocomplete",
    "provider": "ollama",
    "model": "qwen2.5-coder:70b",
    "apiBase": "http://localhost:11434"
  },
  "allowAnonymousTelemetry": false,
  "enableTabAutocomplete": true
}
EOF
```

---

## ğŸš€ Krok 5: VS Code Settings pro AI Workflow

### **Aktualizace settings.json**
```bash
# OtevÅ™i VS Code settings
code ~/.config/Code/User/settings.json

# PÅ™idej nebo aktualizuj nÃ¡sledujÃ­cÃ­ nastavenÃ­:
```

```json
{
  "editor.formatOnSave": true,
  "editor.defaultFormatter": "esbenp.prettier-vscode",
  "editor.codeActionsOnSave": {
    "source.fixAll.eslint": true,
    "source.organizeImports": true
  },
  "editor.suggestSelection": "first",
  "editor.tabCompletion": "on",
  "editor.wordBasedSuggestions": "off",
  
  "continue.telemetryEnabled": false,
  "continue.enableTabAutocomplete": true,
  
  "files.associations": {
    "*.aider": "markdown",
    "*.mcp": "json",
    "*.prompt": "markdown"
  },
  
  "terminal.integrated.defaultProfile.linux": "zsh",
  "terminal.integrated.fontFamily": "MesloLGS NF",
  
  "workbench.colorCustomizations": {
    "activityBar.background": "#1a1a2e",
    "statusBar.background": "#16213e", 
    "titleBar.activeBackground": "#1a1a2e",
    "terminal.background": "#0c0c0c"
  },
  
  "typescript.preferences.includePackageJsonAutoImports": "auto",
  "javascript.preferences.includePackageJsonAutoImports": "auto",
  
  "eslint.workingDirectories": ["./"],
  "prettier.requireConfig": false,
  "prettier.useEditorConfig": false,
  
  "git.autofetch": true,
  "git.confirmSync": false,
  
  "python.defaultInterpreterPath": "/usr/bin/python3.11",
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": true,
  
  "[javascript]": {
    "editor.defaultFormatter": "esbenp.prettier-vscode"
  },
  "[typescript]": {
    "editor.defaultFormatter": "esbenp.prettier-vscode"
  },
  "[json]": {
    "editor.defaultFormatter": "esbenp.prettier-vscode"
  },
  "[python]": {
    "editor.defaultFormatter": "ms-python.black-formatter"
  }
}
```

---

## ğŸš€ Krok 6: VS Code Tasks pro AI Workflow

### **VytvoÅ™enÃ­ tasks.json**
```bash
# VytvoÅ™ .vscode adresÃ¡Å™ v home directory pro globÃ¡lnÃ­ tasks
mkdir -p ~/.vscode

# VytvoÅ™ tasks.json
cat > ~/.vscode/tasks.json << 'EOF'
{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "ğŸ¤– Start AI Models",
      "type": "shell",
      "command": "bash",
      "args": ["-c", "echo 'ğŸš€ Starting AI Models...' && ollama serve"],
      "group": "build",
      "presentation": {
        "echo": true,
        "reveal": "always",
        "panel": "new",
        "showReuseMessage": false
      },
      "isBackground": true,
      "problemMatcher": []
    },
    {
      "label": "ğŸ“Š AI Models Status",
      "type": "shell",
      "command": "bash",
      "args": ["-c", "echo 'ğŸ“Š AI Models Status:' && ollama list && echo '\\nğŸ”¥ GPU Status:' && nvidia-smi --query-gpu=name,memory.used,memory.total,utilization.gpu --format=csv,noheader,nounits"],
      "group": "test",
      "presentation": {
        "echo": true,
        "reveal": "always",
        "panel": "new"
      }
    },
    {
      "label": "ğŸš€ Full-Stack Project Setup",
      "type": "shell", 
      "command": "bash",
      "args": ["-c", "echo 'ğŸš€ Setting up full-stack project...' && npx create-next-app@latest ${input:projectName} --typescript --tailwind --eslint --app && cd ${input:projectName} && npm install @prisma/client prisma && npx prisma init"],
      "group": "build",
      "presentation": {
        "echo": true,
        "reveal": "always",
        "panel": "new"
      }
    },
    {
      "label": "ğŸ”§ Deploy to Railway + Vercel",
      "type": "shell",
      "command": "bash", 
      "args": ["-c", "echo 'ğŸ”§ Deploying to production...' && railway deploy && vercel --prod"],
      "group": "build",
      "dependsOrder": "sequence"
    },
    {
      "label": "ğŸ§ª Run AI Tests",
      "type": "shell",
      "command": "bash",
      "args": ["-c", "echo 'ğŸ§ª Running AI-enhanced tests...' && npm test -- --coverage"],
      "group": "test"
    },
    {
      "label": "ğŸ“ˆ Performance Monitor",
      "type": "shell",
      "command": "watch",
      "args": ["-n", "2", "bash -c 'echo \"=== GPU Status ===\"; nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits; echo \"\\n=== Ollama Models ===\"; ollama ps'"],
      "group": "test",
      "isBackground": true
    }
  ],
  "inputs": [
    {
      "id": "projectName",
      "description": "NÃ¡zev novÃ©ho projektu",
      "default": "my-ai-project"
    }
  ]
}
EOF
```

---

## ğŸš€ Krok 7: VytvoÅ™enÃ­ AI Workflow Scriptu

### **Startup script pro AI modely**
```bash
# VytvoÅ™ scripts adresÃ¡Å™
mkdir -p ~/ai-scripts

# VytvoÅ™ startup script
cat > ~/ai-scripts/start-ai-workflow.sh << 'EOF'
#!/bin/bash

echo "ğŸš€ Starting AI Development Workflow..."

# Start Ollama service
sudo systemctl start ollama
sleep 3

# Check if models are available
echo "ğŸ“‹ Checking AI models..."
ollama list

# Start Continue.dev compatible server
echo "ğŸ”§ Starting MCP server..."
cd ~/ai-scripts
nohup python3 mcp-server.py > mcp.log 2>&1 &

# Check GPU status
echo "ğŸ”¥ GPU Status:"
nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits

echo "âœ… AI Workflow ready!"
echo "ğŸ’¡ Tip: Open VS Code and press Ctrl+Shift+P â†’ 'Continue: Chat'"
EOF

chmod +x ~/ai-scripts/start-ai-workflow.sh
```

### **JednoduchÃ½ MCP server**
```bash
# VytvoÅ™ zÃ¡kladnÃ­ MCP server
cat > ~/ai-scripts/mcp-server.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import json
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

app = FastAPI(title="AI Workflow MCP Server")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health():
    return {"status": "healthy", "message": "AI Workflow MCP Server running"}

@app.get("/api/models")
async def get_models():
    return {
        "models": [
            {"name": "qwen2.5:70b", "type": "architect", "status": "available"},
            {"name": "qwen2.5-coder:70b", "type": "coder", "status": "available"}, 
            {"name": "deepseek-v3", "type": "analyst", "status": "available"}
        ]
    }

@app.websocket("/ws/ai")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    await websocket.send_text(json.dumps({
        "type": "connection", 
        "message": "AI Workflow connected"
    }))
    
    try:
        while True:
            data = await websocket.receive_text()
            # Echo back for now - extend with actual AI processing
            await websocket.send_text(json.dumps({
                "type": "response",
                "message": f"Received: {data}"
            }))
    except:
        pass

if __name__ == "__main__":
    print("ğŸš€ Starting AI Workflow MCP Server on http://localhost:3001")
    uvicorn.run(app, host="0.0.0.0", port=3001)
EOF

chmod +x ~/ai-scripts/mcp-server.py
```

---

## ğŸš€ Krok 8: Test Installation

### **OvÄ›Å™enÃ­ instalace**
```bash
# 1. Start AI workflow
~/ai-scripts/start-ai-workflow.sh

# 2. Test Ollama models
ollama run qwen2.5-coder:70b "NapiÅ¡ hello world v Reactu"

# 3. Test Continue.dev v VS Code
code
# Stiskni Ctrl+Shift+P â†’ "Continue: Chat"
# Zadej: "VytvoÅ™ React komponentu pro user profile card"

# 4. Test MCP server
curl http://localhost:3001/health
curl http://localhost:3001/api/models
```

### **Test React projektu s AI**
```bash
# VytvoÅ™ testovacÃ­ projekt
mkdir ~/test-ai-project
cd ~/test-ai-project

# Iniciuj Next.js projekt
npx create-next-app@latest . --typescript --tailwind --eslint --app

# OtevÅ™i v VS Code
code .

# Test AI promptu v Continue.dev:
# "VytvoÅ™ dashboard komponentu s PostgreSQL pÅ™ipojenÃ­m"
```

---

## ğŸš€ Krok 9: Integrace s Existing Database

### **Konfigurace Prisma pro PostgreSQL**
```bash
# V jakÃ©mkoli projektu
npm install prisma @prisma/client

# VytvoÅ™ .env soubor s tvÃ½m DigitalOcean PostgreSQL
cat > .env << 'EOF'
DATABASE_URL="postgresql://<username>:<password>@<host>:<port>/<database>?sslmode=require"
EOF

# Iniciuj Prisma
npx prisma init

# Test pÅ™ipojenÃ­
npx prisma db pull
```

---

## ğŸš€ Krok 10: VS Code Launch Configuration

### **VytvoÅ™enÃ­ launch.json pro debugging**
```bash
# VytvoÅ™ globÃ¡lnÃ­ launch configuration
mkdir -p ~/.vscode

cat > ~/.vscode/launch.json << 'EOF'
{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "ğŸš€ Debug Next.js App",
      "type": "node",
      "request": "launch",
      "program": "${workspaceFolder}/node_modules/.bin/next",
      "args": ["dev"],
      "console": "integratedTerminal",
      "skipFiles": ["<node_internals>/**"]
    },
    {
      "name": "ğŸ§ª Debug Node.js API",
      "type": "node",
      "request": "launch",
      "program": "${workspaceFolder}/server.js",
      "env": {
        "NODE_ENV": "development"
      },
      "console": "integratedTerminal"
    },
    {
      "name": "ğŸ Debug Python Script",
      "type": "python",
      "request": "launch",
      "program": "${file}",
      "console": "integratedTerminal",
      "justMyCode": true
    }
  ]
}
EOF
```

---

## âœ… FinÃ¡lnÃ­ OvÄ›Å™enÃ­

### **Checklist ÃºspÄ›Å¡nÃ© instalace:**
- [ ] Ollama modely bÄ›Å¾Ã­ (`ollama list`)
- [ ] Continue.dev funguje v VS Code (`Ctrl+Shift+P` â†’ Continue: Chat)
- [ ] MCP server odpovÃ­dÃ¡ (`curl localhost:3001/health`)
- [ ] GPU je dostupnÃ¡ pro AI (`nvidia-smi`)
- [ ] PostgreSQL pÅ™ipojenÃ­ funguje (`psql <connection-string>`)
- [ ] React projekt se vytvÃ¡Å™Ã­ a spouÅ¡tÃ­ (`npm run dev`)
- [ ] AI generuje funkÄnÃ­ kÃ³d v Continue.dev

### **Quick Test Prompt:**
```
"VytvoÅ™ kompletnÃ­ React dashboard aplikaci s:
- TypeScript a Tailwind CSS
- PostgreSQL databÃ¡ze pÅ™es Prisma
- User authentication
- Data visualization charts
- Responsive design
- Production deployment config

PouÅ¾ij modernÃ­ best practices a zahrÅˆ error handling."
```

---

## ğŸ¯ Co MÃ¡Å¡ Po Instalaci

### **DostupnÃ© AI Modely:**
- ğŸ¯ **Qwen 3 2507** - Architekt (plÃ¡novÃ¡nÃ­, design)
- ğŸ’» **Qwen2.5-Coder 70B** - HlavnÃ­ programÃ¡tor
- ğŸ§  **DeepSeek V3** - Code reviewer a analytik

### **VS Code Funkce:**
- AI chat integrovanÃ½ pÅ™Ã­mo v editoru
- Tab autocomplete s AI
- Custom commands pro bÄ›Å¾nÃ© Ãºkoly
- AutomatickÃ© formÃ¡tovÃ¡nÃ­ a linting
- Debugging konfigurace
- Task automation

### **Workflow Capabilities:**
- GenerovÃ¡nÃ­ celÃ½ch projektÅ¯ jednÃ­m promptem
- AutomatickÃ© pÅ™ipojenÃ­ k PostgreSQL
- Deployment na Railway + Vercel
- Code review a optimalizace
- Security audit
- Performance analysis

---

## ğŸ“ Troubleshooting

### **Pokud nÄ›co nefunguje:**
```bash
# Restart Ollama
sudo systemctl restart ollama

# Check GPU
nvidia-smi

# Check models
ollama list

# Restart VS Code
code --disable-extensions && code --enable-extensions

# Check logs
journalctl -u ollama -f
```

### **ÄŒastÃ© problÃ©my:**
1. **Ollama neodpovÃ­dÃ¡**: `sudo systemctl restart ollama`
2. **GPU nedostupnÃ¡**: Zkontroluj NVIDIA ovladaÄe
3. **Continue.dev nefunguje**: Restartuj VS Code
4. **Model pÅ™Ã­liÅ¡ pomalÃ½**: Zkontroluj GPU utilization

---

## ğŸš€ Ready to Go!

Po dokonÄenÃ­ tÄ›chto krokÅ¯ budeÅ¡ mÃ­t nejpokroÄilejÅ¡Ã­ AI development environment, kterÃ©:

- **PÅ™ekonÃ¡ Replit** rychlostÃ­ a funkcemi
- **NahradÃ­ Cursor** s vlastnÃ­mi modely  
- **Funguje offline** nezÃ¡visle na internetu
- **StojÃ­ $0** mÄ›sÃ­ÄnÄ› vs $20+ u konkurence
- **DÃ¡vÃ¡ plnou kontrolu** nad kÃ³dem a daty

**ZaÄni s jednoduchÃ½m promptem a pozoruj, jak AI vytvÃ¡Å™Ã­ celÃ© aplikace! ğŸš€**